from app.services.parser import load_document
from app.services.chunker import chunk_text
from app.services.embedder import Embedder 
from app.db.faiss_db import FAISSVectorStore
from app.core.rag_pipeline import RAGPipeline

import os
from dotenv import load_dotenv

load_dotenv()

# Global history maintained throughout conversation
chat_history = []

def initialize_vector_db(file_path: str):
    print("ğŸ“„ Reading document...")
    text = load_document(file_path)
    print("ğŸ“‚ Type of loaded text:", type(text))

    print("âœ‚ï¸ Chunking document...")
    chunks = chunk_text(text)
    print(f"ğŸ§© Total chunks: {len(chunks)}")

    print("ğŸ”— Creating embeddings...")
    embedder = Embedder()
    embedded_chunks = embedder.embed_chunks(chunks)

    print("ğŸ“¦ Saving to FAISS DB...")
    store = FAISSVectorStore()
    store.add_embeddings(embedded_chunks)

    print("âœ… Document processed and stored in vector DB.")


def chat_with_bot():
    rag = RAGPipeline()

    while True:
        question = input("You ğŸ§ : ")

        if question.lower() in ["exit", "quit"]:
            print("ğŸ‘‹ Goodbye!")
            break

        answer = rag.get_answer(question, chat_history=chat_history)

        chat_history.append({"user": question, "assistant": answer})
        print(f"Bot ğŸ¤–: {answer}\n")


if __name__ == "__main__":
    FILE_PATH = "data/journal_entries.txt"  # replace with actual path
    initialize_vector_db(FILE_PATH)
    chat_with_bot()
